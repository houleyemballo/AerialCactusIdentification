{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://ars.els-cdn.com/content/image/1-s2.0-S1574954119300895-gr1_lrg.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Overview","metadata":{}},{"cell_type":"markdown","source":"With the effects of climate change impacting the globe now more than ever, it is critical to maintain healthy natural areas as much as possible. Human activity has a large impact on environmental health, and tracking that impact is critical. Activities like logging, mining, and agriculture can destroy the functioning of natural areas.  In each ecosystem, there are plants and animals that have an outsized impact on the healthy balance found in the region. These keystone species are critical to the survival of many other species in the region. Performing an accurate account of the species present can give insight into the health of a given area, and can help pinpoint areas that are in special need of immediate restoration efforts. \n\nFor an in depth look into the original research, please reference the paper found [here.](https://www.sciencedirect.com/science/article/abs/pii/S1574954119300895?via%3Dihub)","metadata":{}},{"cell_type":"markdown","source":"# Business Understanding","metadata":{}},{"cell_type":"markdown","source":"![](https://live.staticflickr.com/2251/5758564003_70a47ae0eb.jpg)","metadata":{}},{"cell_type":"markdown","source":"[](https://live.staticflickr.com/5181/5758564919_8fb1db830e_b.jpg)","metadata":{}},{"cell_type":"markdown","source":"The Mexican Government has partnered with the VIGIA project, which aims to create an autonomous surveillance system for monitoring the ecological health of protected natural areas. The first step in this project is to use drone footage to identify critical, or keystone species in the images. The initial images are from the Tehuacán-Cuicatlán Valley, a semi-arid zone in southern Mexico. This area was chosen because of its wide biodiversity and the importance that the ecology plays in the health of the region. It became a UNESCO World Heritage Site in 2018 in an effort to protect the area. Despite this protection, human activity is still impacting the health of the region. \n\n## Ecological Understanding\n\nOur work focuses on columnar cactus recognition as they are a keystone species in the region. A keystone species is defined as an organism that helps to hold the system together. Without this species, the ecosystem would look different and its functions would shift dramatically. Some ecosystems may not be able to adapt to environmental changes if the keystone species disappears. With the backbone of the area gone, there is room for invasive species to take over and dramatically shift the ecosystem in an unhealthy direction. \n\nIn the Tehuacán-Cuicatlán Valley, some of the richest biodiversity in North America is found. Due to the nearby mountains, the valley only receives annual precipitations of 400 to 800mm. A central feature in this diverse region is the columnar cactus. It provides essential habitat for many species of animals. The columnar cactus also functions to reduce erosion in the area and helps to maintain vital water in the landscape. As the cactus disappears, many other species of birds, lizards, mammals, and insects lose their habitat and food and water source. Research has also found the columnar cactus provides important features in the landscape that provides ideal germination grounds for many other species of plants endemic to the region.\n\nHuman activity has reduced the healthy land in the valley by 14.69 percent since the year 2000. Agricultural activities, along with mining and deforestation are impacting the health of the region. Because of the large size of the site, it is nearly impossible to monitor the entire area, leaving many remote spaces vulnerable to degradation.\n\nFor many years, aerial vehicles like small airplanes and helicopters have been used to monitor large natural areas. The cost for using these vehicles is high, which limits the frequency of use. As drones have become easier to access and cheap to use, they can be applied to unique problems like natural ecosystem monitoring. They can be used for frequently, allowing for more images to be collected and processed for a deeper understanding of the region.\n\n\n# False Negatives and Positives \n\nIn our model, we will be assessing if an image has a columnar cactus present or not. A false negative is an image that has a cactus present, but is incorrectly labeled as not holding a cactus. A false positive is an image that is predicted to have a cactus present when it does not.\n\nCare must be taken to ensure that images labeled as having cactus present are accurate. While it would cause a waste of labor to move to intervene in an area that was incorrectly marked as not having cacti, it would be more problematic to miss degraded areas with no cactus all together. This would mean areas that are in need of restoration would go unnoticed.","metadata":{}},{"cell_type":"code","source":"#Import Relevant Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Input, LSTM, AveragePooling2D\nfrom keras.datasets import mnist\nfrom keras import regularizers, initializers, optimizers\nimport os\nimport datetime\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import plot_confusion_matrix\nfrom pathlib import Path\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix\nimport seaborn as sns\nimport gc\nfrom timeit import default_timer as timer\nimport tensorflow as tf\npd.set_option('display.float_format', lambda x: '%.1f' % x)\nos.environ['KMP_DUPLICATE_LIB_OK']='True' #This prevents kernel shut down due to xgboost conflict\nfrom tensorflow.keras.preprocessing import image\nimport zipfile\nimport random\nimport cv2\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.layers import BatchNormalization, Reshape, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport pydot\nimport graphviz\nfrom tensorflow.keras.utils import plot_model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Take a look at the available data\nprint(os.listdir(\"../input/aerial-cactus-identification/\"))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:06:50.842665Z","iopub.execute_input":"2021-12-16T21:06:50.842983Z","iopub.status.idle":"2021-12-16T21:06:59.370836Z","shell.execute_reply.started":"2021-12-16T21:06:50.842901Z","shell.execute_reply":"2021-12-16T21:06:59.369923Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ndef plot_performance(hist,model):\n    \"\"\" Returns 5 pairs of plots comparing Training and Validation data. \n    The first column is autoscaled. The second column has a set scale.\n    First pair of plots returns training and validation accuracy. \n    Second pair of plots returns training and validation loss. \n    Third pair of plots returns training and validation F1-Scores. \n    Fourth pair of plots returns training and validation recall scores. \n    Fifth pair of plots returns training and validation precision scores.\n    \n    hist: input history model containing train images, labels, and validation data. \"\"\"\n    \n    hist_ = hist.history\n    epochs = hist.epoch\n    recall = np.array(hist_['recall'])\n    precision = np.array(hist_['precision'])\n    val_recall = np.array(hist_['val_recall'])\n    val_precision = np.array(hist_['val_precision'])\n    \n    #\n    figure,axis = plt.subplots(5,2, figsize = (30,30))\n    #Accuracy plots\n    axis[0,0].plot(epochs, hist_['accuracy'], label='Training Accuracy', color = 'xkcd:plum')\n    axis[0,0].plot(epochs, hist_['val_accuracy'], label='Validation Accuracy', color = 'xkcd:aqua')\n    axis[0,0].set_xlabel('Epochs')\n    axis[0,0].set_ylabel('Score')\n    axis[0,0].set_title('Training and validation accuracy')\n    axis[0,0].legend()\n    \n    \n    axis[0,1].plot(epochs, hist_['accuracy'], label='Training Accuracy', color = 'xkcd:plum')\n    axis[0,1].plot(epochs, hist_['val_accuracy'], label='Validation Accuracy', color = 'xkcd:aqua')\n    axis[0,1].set_ylim(0,1)\n    axis[0,1].set_xlabel('Epochs')\n    axis[0,1].set_ylabel('Score')\n    axis[0,1].set_title('Training and validation accuracy')\n    axis[0,1].legend()\n    \n    \n    #--------------------------------------------------------------------\n    #Loss\n    axis[1,0].plot(epochs, hist_['loss'], label='Training loss', color = 'xkcd:plum')\n    axis[1,0].plot(epochs, hist_['val_loss'], label='Validation loss', color = 'xkcd:aqua')\n    axis[1,0].set_xlabel('Epochs')\n    axis[1,0].set_ylabel('Score')\n    axis[1,0].set_title('Training and validation loss')\n    axis[1,0].legend()\n    \n    \n    axis[1,1].plot(epochs, hist_['loss'], label='Training loss', color = 'xkcd:plum')\n    axis[1,1].plot(epochs, hist_['val_loss'], label='Validation loss', color = 'xkcd:aqua')\n    axis[1,1].set_xlabel('Epochs')\n    axis[1,1].set_ylabel('Score')\n    axis[1,1].set_ylim(0,1)\n    axis[1,1].set_title('Training and validation loss')\n    axis[1,1].legend()\n    \n    #-------------------------------------------------------------------\n    #F1 Scores\n    axis[2,0].plot(epochs, \n             2*((recall * precision)/(recall + precision)), \n             label='Training f1', color = 'xkcd:plum')\n    axis[2,0].plot(epochs, \n             2*((val_recall * val_precision)/(val_recall + val_precision)), \n             label='Validation f1', color = 'xkcd:aqua')\n    axis[2,0].set_xlabel('Epochs')\n    axis[2,0].set_ylabel('Score')\n    axis[2,0].set_title('Training and validation F1-Score')\n    axis[2,0].legend()\n    \n    \n    axis[2,1].plot(epochs, \n             2*((recall * precision)/(recall + precision)), \n             label='Training f1', color = 'xkcd:plum')\n    axis[2,1].plot(epochs, \n             2*((val_recall * val_precision)/(val_recall + val_precision)), \n             label='Validation f1', color = 'xkcd:aqua')\n    axis[2,1].set_xlabel('Epochs')\n    axis[2,1].set_ylabel('Score')\n    axis[2,1].set_ylim(0,1)\n    axis[2,1].set_title('Training and validation F1-Score')\n    axis[2,1].legend()\n    \n    #-------------------------------------------------------------------------\n    #Recall\n    \n    \n    axis[3,0].plot(epochs, recall, label = \"Training Recall\", color = 'xkcd:plum')\n    axis[3,0].plot(epochs, val_recall, label = \"Validation Recall\", color = 'xkcd:aqua')\n    axis[3,0].set_xlabel('Epochs')\n    axis[3,0].set_ylabel('Score')\n    axis[3,0].set_title(\"Training and Validation Recall Scores\")\n    axis[3,0].legend()\n    \n    \n    axis[3,1].plot(epochs, recall, label = \"Training Recall\", color = 'xkcd:plum')\n    axis[3,1].plot(epochs, val_recall, label = \"Validation Recall\", color = 'xkcd:aqua')\n    axis[3,1].set_ylim(0,1)\n    axis[3,1].set_xlabel('Epochs')\n    axis[3,1].set_ylabel('Score')\n    axis[3,1].set_title(\"Training and Validation Recall Scores\")\n    axis[3,1].legend()\n    \n    #------------------------------------------------------------------------\n    #Precision\n    axis[4,0].plot(epochs, precision, label = \"Training Precision\", color = 'xkcd:plum')\n    axis[4,0].plot(epochs, val_precision, label = \"Validation Precision\", color = 'xkcd:aqua')\n    axis[4,0].set_xlabel('Epochs')\n    axis[4,0].set_ylabel('Score') \n    axis[4,0].set_title(\"Training and Validation Precision Scores\")\n    axis[4,0].legend()\n    \n    \n    \n    axis[4,1].plot(epochs, precision, label = \"Training Precision\", color = 'xkcd:plum')\n    axis[4,1].plot(epochs, val_precision, label = \"Validation Precision\", color = 'xkcd:aqua')\n    axis[4,1].set_xlabel('Epochs')\n    axis[4,1].set_ylabel('Score')\n    axis[4,1].set_ylim(0,1)\n    axis[4,1].set_title(\"Training and Validation Precision Scores\")\n    axis[4,1].legend()\n    \n    \n    plt.show()\n    \n    score = model.evaluate(val_generator, verbose=0)\n    print(\"Test loss:\", score[0])\n    print(\"Test accuracy:\", score[1])\n    print(\"Test recall:\", score[2])\n    print(\"Test precision:\", score[3])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:06:59.373019Z","iopub.execute_input":"2021-12-16T21:06:59.373530Z","iopub.status.idle":"2021-12-16T21:06:59.410790Z","shell.execute_reply.started":"2021-12-16T21:06:59.373483Z","shell.execute_reply":"2021-12-16T21:06:59.409791Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding","metadata":{}},{"cell_type":"markdown","source":"The original data found in this notebook was generated by the research team, VIGIA. VIGIA stands for Vigilancia Autónoma de Reservas de la Biósfera, or Autonomous Surveillance of Biosphere Reserves. Using drone footage, the team collected over 17,000 images of a 10,000 square km area of the Tehuacán-Cuicatlán Valley in Southern Mexico. The images were taken by a DJI Phantom 3 Advanced drone flying at 100m. The images were manually identified and marked by the research team in partnership with the Mexican Government.  The goal of the project was to use drone footage to assist in the ecosystem monotoring necessary for sensitive environmental areas.\n\nThe dataset conatins a collection of 32 x 32 thumbnail images. The images used in thie notebook were resized but the original images can be found [here.](https://www.kaggle.com/irvingvasquez/cactus-aerial-photos) The original images were pulled at 5s intervals using a camera with a resolution of 2704 X 1520 pixels. The patches with cacti were isolated and saved in a 32 X 32 pixel format. \n\n","metadata":{}},{"cell_type":"markdown","source":"First, we exract our images from their zip files, and save them to a temporary directory for ease of working. We then can take a look at how many images are present in both our train and test files. ","metadata":{}},{"cell_type":"code","source":"# Extract images from zip files\n\nwith zipfile.ZipFile(\"../input/aerial-cactus-identification/train.zip\",\"r\") as z:\n    z.extractall(\"/kaggle/temp/\")\nwith zipfile.ZipFile(\"../input/aerial-cactus-identification/test.zip\",\"r\") as z:\n    z.extractall(\"/kaggle/temp/test/\") # needs to be in subdirectory (i.e. test/test/) for flow_from_directory to work\n\n\n#Looking at the number of images in our train and test files\nprint(len(os.listdir(\"../temp/train\")))\nprint(len(os.listdir(\"../temp/test/test\")))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:06:59.412164Z","iopub.execute_input":"2021-12-16T21:06:59.412393Z","iopub.status.idle":"2021-12-16T21:07:03.079000Z","shell.execute_reply.started":"2021-12-16T21:06:59.412365Z","shell.execute_reply":"2021-12-16T21:07:03.078081Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Setting up our working directories\ntrain_dir = \"../temp/train\"\n#The test directory is reserved for the final competition submission\ntest_dir = \"../temp/test\"\nlabels = pd.read_csv('../input/aerial-cactus-identification/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:03.079995Z","iopub.execute_input":"2021-12-16T21:07:03.080190Z","iopub.status.idle":"2021-12-16T21:07:03.124182Z","shell.execute_reply.started":"2021-12-16T21:07:03.080164Z","shell.execute_reply":"2021-12-16T21:07:03.123416Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Image Analysis Overview","metadata":{}},{"cell_type":"markdown","source":"We can see from the count, and visualized in the graph below, the classes are very imbalanced. There are three times as many cacti images as there are no cacti images. We will later address this imbalance by augmenting the photos.","metadata":{}},{"cell_type":"code","source":"#Taking a look at the class balance \nlabels.has_cactus = labels.has_cactus.astype(str)\nprint(labels['has_cactus'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:03.129080Z","iopub.execute_input":"2021-12-16T21:07:03.129640Z","iopub.status.idle":"2021-12-16T21:07:03.182368Z","shell.execute_reply.started":"2021-12-16T21:07:03.129587Z","shell.execute_reply":"2021-12-16T21:07:03.181505Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Creating a simple plot of the data to show imabalance\nsns.histplot(data = labels['has_cactus'], shrink = .8)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:03.183782Z","iopub.execute_input":"2021-12-16T21:07:03.184618Z","iopub.status.idle":"2021-12-16T21:07:03.456135Z","shell.execute_reply.started":"2021-12-16T21:07:03.184569Z","shell.execute_reply":"2021-12-16T21:07:03.455309Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"A random sample of the images can give us insight into the quality of the images.  We can see that the images are not of the highest resolution which is to be expected from images taken from 100m, but the cacti are distinct. Because we are working with drone footage, the cacti will be represented in many different orientations. We can use this aspect of drone footage to our advantage later when augmenting the photos. We do not have to be concerned with the directions of the photos so we will be able to flip them in different directions which can help strengthen our model.","metadata":{}},{"cell_type":"markdown","source":"This segment of code was sourced from this [kaggle user](https://www.kaggle.com/jacobmorrison213/simple-cnn-using-keras) to look through a random set of images. ","metadata":{}},{"cell_type":"code","source":"# Plot random sample of training images\n\nrand_images = random.sample(os.listdir(train_dir), 16)\n\nfig = plt.figure(figsize=(16,4))\nfor i, im in enumerate(rand_images):\n    plt.subplot(2, 8, i+1)\n    im = cv2.imread(os.path.join(train_dir, im))\n    plt.imshow(im)\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:03.457535Z","iopub.execute_input":"2021-12-16T21:07:03.458256Z","iopub.status.idle":"2021-12-16T21:07:04.486538Z","shell.execute_reply.started":"2021-12-16T21:07:03.458208Z","shell.execute_reply":"2021-12-16T21:07:04.485516Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#Creating a sample split to look at example images with their corresponding labels.\nex_split = 0.8\nindex = np.random.permutation(range(len(labels))) < ex_split*len(labels)\n\nex_labels = labels[index]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:04.487955Z","iopub.execute_input":"2021-12-16T21:07:04.488182Z","iopub.status.idle":"2021-12-16T21:07:04.507046Z","shell.execute_reply.started":"2021-12-16T21:07:04.488155Z","shell.execute_reply":"2021-12-16T21:07:04.505910Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ex_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255, horizontal_flip=True, vertical_flip=True)\n\nbatch_size = 128\n\nex_generator = ex_datagen.flow_from_dataframe(ex_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:04.508564Z","iopub.execute_input":"2021-12-16T21:07:04.508967Z","iopub.status.idle":"2021-12-16T21:07:04.690924Z","shell.execute_reply.started":"2021-12-16T21:07:04.508932Z","shell.execute_reply":"2021-12-16T21:07:04.690283Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ex_images, ex_labels = next(ex_generator)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:04.691990Z","iopub.execute_input":"2021-12-16T21:07:04.692326Z","iopub.status.idle":"2021-12-16T21:07:04.732091Z","shell.execute_reply.started":"2021-12-16T21:07:04.692293Z","shell.execute_reply":"2021-12-16T21:07:04.731272Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"In the images below, we can see an example of both an image with a cactus present, and an image with no cactus present.","metadata":{}},{"cell_type":"code","source":"plt.imshow(array_to_img(ex_images[0]))\nprint(ex_labels[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:04.733242Z","iopub.execute_input":"2021-12-16T21:07:04.733634Z","iopub.status.idle":"2021-12-16T21:07:04.950799Z","shell.execute_reply.started":"2021-12-16T21:07:04.733594Z","shell.execute_reply":"2021-12-16T21:07:04.949777Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.imshow(array_to_img(ex_images[3]))\nprint(ex_labels[3])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:04.951962Z","iopub.execute_input":"2021-12-16T21:07:04.952655Z","iopub.status.idle":"2021-12-16T21:07:05.172606Z","shell.execute_reply.started":"2021-12-16T21:07:04.952620Z","shell.execute_reply":"2021-12-16T21:07:05.171655Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Performing a Train Test Split","metadata":{}},{"cell_type":"markdown","source":"Here we split our data into our training set and our validation set. 80% of the images are going into the training set with the remaining images being used to validate our models performance.","metadata":{}},{"cell_type":"code","source":"#Splits the data randomly\nval_split = 0.8\nindex = np.random.permutation(range(len(labels))) < val_split*len(labels)\n\ntrain_labels = labels[index]\nval_labels = labels[~index]\nprint(len(train_labels), len(val_labels))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.173884Z","iopub.execute_input":"2021-12-16T21:07:05.174536Z","iopub.status.idle":"2021-12-16T21:07:05.195223Z","shell.execute_reply.started":"2021-12-16T21:07:05.174496Z","shell.execute_reply":"2021-12-16T21:07:05.194153Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Image Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"We want to start pre-processing our images before putting them through the models. Our pre-processing will ensure all images are the same size and are scaled down to relieve any computational pressure. In the initial image data generator, we flip our images horizontally and vertically to give our model more images from which to train. We then use flow from dataframe to utilize the augmentation from our image data generator.\n\nAs we set up our train and validation generators, we have to make sure to specify our batch size and target size. It is important as well to indicate that we are working on a binary classification problem by setting class_mode to 'binary'.","metadata":{}},{"cell_type":"code","source":"# Process image JPEGs into tensors\n# Pixel values rescaled from [0,255] to [0,1]\n\n# Generate batches of tensor image data (with real-time data augmentation - horizontal and vertical flips)\ntrain_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255, horizontal_flip=True, vertical_flip=True)\n\nbatch_size = 128\n\ntrain_generator = train_datagen.flow_from_dataframe(train_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))\n\nval_generator = train_datagen.flow_from_dataframe(val_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))\n\ntest_generator = train_datagen.flow_from_dataframe(val_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.198771Z","iopub.execute_input":"2021-12-16T21:07:05.199015Z","iopub.status.idle":"2021-12-16T21:07:05.479900Z","shell.execute_reply.started":"2021-12-16T21:07:05.198984Z","shell.execute_reply":"2021-12-16T21:07:05.478962Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We can now set our input shape that will be refelective of the target_size given above, and also the number of layers found in our images. Because we are working with RGB images, our layers are set to 3.","metadata":{}},{"cell_type":"code","source":"#setting input shape to be used in all models\ninput_shape = (32,32,3)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.481050Z","iopub.execute_input":"2021-12-16T21:07:05.481295Z","iopub.status.idle":"2021-12-16T21:07:05.485141Z","shell.execute_reply.started":"2021-12-16T21:07:05.481265Z","shell.execute_reply":"2021-12-16T21:07:05.484491Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Model 1: Simple Baseline Model\n","metadata":{}},{"cell_type":"markdown","source":"In the first model, we would like to start simply to see what kind of results we can get with a little complication as possible. \n\nWe start with an input layer to establish the structure of the incoming data. We then include a flattening layer to set our images into one long array. Two simple dense layers are then added which funnel the information down. The first dense layer uses a relu activation function. Our final dense layer uses a sigmoid activation function because we are working on a binary classification problem.","metadata":{}},{"cell_type":"code","source":"#First simple model\nmodel_start = Sequential(\n    [\n        Input(input_shape),\n        Flatten(), # need to flatten our images to be one long array\n        Dense(64,activation=\"relu\"),\n        Dense(1, activation=\"sigmoid\"),  \n        \n    ])\n\nmodel_start.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.486168Z","iopub.execute_input":"2021-12-16T21:07:05.487074Z","iopub.status.idle":"2021-12-16T21:07:05.618324Z","shell.execute_reply.started":"2021-12-16T21:07:05.487036Z","shell.execute_reply":"2021-12-16T21:07:05.617525Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We now compile our model using binary crossentropy to evaluate our loss. We also use adam as our optimizer and evaluate using the accuracy, recall and precision metrics.. ","metadata":{}},{"cell_type":"code","source":"model_start.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy', 'Recall', 'Precision'])","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.619527Z","iopub.execute_input":"2021-12-16T21:07:05.619768Z","iopub.status.idle":"2021-12-16T21:07:05.634397Z","shell.execute_reply.started":"2021-12-16T21:07:05.619738Z","shell.execute_reply":"2021-12-16T21:07:05.633439Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We now can move to fitting our baseline model. We will set the number of epochs to 20 to get a quick understanding of our models performance. ","metadata":{}},{"cell_type":"code","source":"history = model_start.fit(train_generator, \n                     batch_size=batch_size,\n                     epochs=20, \n                     validation_data=val_generator,\n                     verbose = 2    \n                         )","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:07:05.635821Z","iopub.execute_input":"2021-12-16T21:07:05.636692Z","iopub.status.idle":"2021-12-16T21:09:29.985609Z","shell.execute_reply.started":"2021-12-16T21:07:05.636648Z","shell.execute_reply":"2021-12-16T21:09:29.984740Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"plot_performance(history, model_start)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:09:29.986999Z","iopub.execute_input":"2021-12-16T21:09:29.987728Z","iopub.status.idle":"2021-12-16T21:09:33.252171Z","shell.execute_reply.started":"2021-12-16T21:09:29.987692Z","shell.execute_reply":"2021-12-16T21:09:33.251227Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"Our first simple model has an overall test accuracy of 88.97%. Our test recall is 92.64%. Most importantly, our precision is 92.71%. This is a great start for our modeling. Precision is our most important metric because it is evaluating how often an image is accurately reporting that cacti are present. We need to be able to trust this metric because a mislabeling of a land as having cacti will lead to the land getting ignored. ","metadata":{}},{"cell_type":"markdown","source":"# Model Iterations","metadata":{}},{"cell_type":"markdown","source":"In model two, we wanted to play around with diferent types of layers to see their effects.\n\nOur first layer is a Conv2D layer using the relu activation function. Relu is useful over tanh and sigmoid in the topmost layers because it makes the model easier to train and often the model achieves better performance. Setting 'same' to padding extends the area in which CNN processes an image. We also make sure to set our input shape. Our initial filter is set in this layer which determines the number of filters from which the CNN will learn.\n\nWe then include a MaxPooling2D layer. This operation is putting a 2D filter over each channel and summarising the features covered by the filter. This is performing downsampling on our photos, specifically on the height and width of the spatial dimensions.\n\nWe then flatten our matrix down to a single long array before putting it through two more Dense layers.\n\nIn compiling our model we are using an adam optimizer with a set learning rate of .01. Additionally, we have put in an early stop in this model. Early stop will make sure to stop the model learning at a point where it seems there are no more improvements to be made on model function.\n\nWe are giving the next models more epochs to run through to evaluate performance over more iterations. We can set the epoch to a high number, and trust that the early stop will end the model training when no improvements are being made. ","metadata":{}},{"cell_type":"markdown","source":"## Model 2: Adding Convolutional Layers and MaxPooling","metadata":{}},{"cell_type":"code","source":"model2 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\nopt = keras.optimizers.Adam(learning_rate=0.01)\n\n# compiling models\nmodel2.compile(loss='binary_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy', 'Recall', 'Precision'])\n\n# early stopping\ncp = EarlyStopping(patience = 20, restore_best_weights=True)\n\nmodel2.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-16T21:09:33.254287Z","iopub.execute_input":"2021-12-16T21:09:33.254557Z","iopub.status.idle":"2021-12-16T21:09:33.325382Z","shell.execute_reply.started":"2021-12-16T21:09:33.254517Z","shell.execute_reply":"2021-12-16T21:09:33.324491Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"start = timer()\n\nhistory2 = model2.fit(train_generator,\n                    batch_size = batch_size,\n                    epochs=50, \n                    validation_data = val_generator,\n                    callbacks = [cp],\n                    verbose = 2\n                  \n                   )\nend = timer()\nelapsed = end - start\nprint('Total Time Elapsed: ', int(elapsed//60), ' minutes ', (round(elapsed%60)), ' seconds')","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:09:33.326754Z","iopub.execute_input":"2021-12-16T21:09:33.327625Z","iopub.status.idle":"2021-12-16T21:16:04.838936Z","shell.execute_reply.started":"2021-12-16T21:09:33.327579Z","shell.execute_reply":"2021-12-16T21:16:04.837681Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"plot_performance(history2, model2)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:16:04.840227Z","iopub.execute_input":"2021-12-16T21:16:04.840837Z","iopub.status.idle":"2021-12-16T21:16:08.094380Z","shell.execute_reply.started":"2021-12-16T21:16:04.840785Z","shell.execute_reply":"2021-12-16T21:16:08.092140Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Model 2 Evaluation ","metadata":{}},{"cell_type":"markdown","source":"Model 2 has an overall test accuracy of 97.57%. Our test recall is 98.75%. Most importantly, our precision is 98.04%. This is a massive improvement on our first model but we are seeing the recall scores jump all over the place between 91% and 98%. This could be a sign we need to let our models train for longer so in future iterations we will increase the number of epochs to 100. ","metadata":{}},{"cell_type":"markdown","source":"## Model 3: Learning Rate Reductions and Early Stopping","metadata":{}},{"cell_type":"markdown","source":"In model 3, we are keeping the same layers for a our model, but we are making some changes to the early stopping and adding in a feature that allows the learning rate to gradually adjust as needed. We are also going to allow the model to train for 100 epochs. \n\nThe documentation informing the use of the learning rate reduction class can be found [here.](https://keras.io/api/callbacks/reduce_lr_on_plateau/)","metadata":{}},{"cell_type":"code","source":"model3 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\n\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:16:08.095728Z","iopub.execute_input":"2021-12-16T21:16:08.096054Z","iopub.status.idle":"2021-12-16T21:16:08.147761Z","shell.execute_reply.started":"2021-12-16T21:16:08.096018Z","shell.execute_reply":"2021-12-16T21:16:08.146798Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model3.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n#Very cool early stopping and learning rate work\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), # Stop training when a monitored metric has stopped improving\n             ReduceLROnPlateau(patience=10, verbose=1), # Reduce learning rate when a metric has stopped improving\n\n            ]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:16:08.148936Z","iopub.execute_input":"2021-12-16T21:16:08.149360Z","iopub.status.idle":"2021-12-16T21:16:08.161269Z","shell.execute_reply.started":"2021-12-16T21:16:08.149327Z","shell.execute_reply":"2021-12-16T21:16:08.160511Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"history3 = model3.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    \n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:16:08.162583Z","iopub.execute_input":"2021-12-16T21:16:08.162857Z","iopub.status.idle":"2021-12-16T21:27:23.415046Z","shell.execute_reply.started":"2021-12-16T21:16:08.162821Z","shell.execute_reply":"2021-12-16T21:27:23.414150Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plot_performance(history3, model3)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:23.416223Z","iopub.execute_input":"2021-12-16T21:27:23.416484Z","iopub.status.idle":"2021-12-16T21:27:26.341113Z","shell.execute_reply.started":"2021-12-16T21:27:23.416446Z","shell.execute_reply":"2021-12-16T21:27:26.340255Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Model 3 Evaluation","metadata":{}},{"cell_type":"markdown","source":"In model 3 we can see the utility of the learning reduction adjustment. With the same layers and just the addition of early stopping and the learning rate reduction, we have increased our overall accuracy to %99.00. Our recall is now %99.08 and our precision has jumped to %99.43. ","metadata":{}},{"cell_type":"markdown","source":"# Models 4: Additional Maxpooling layers","metadata":{}},{"cell_type":"markdown","source":"In model 4, another pair of Conv2D and MaxPooling layers were added. I also increased the size of the first dense layer. I am making small incremental changes because there is not room for significant improvement at this stage.","metadata":{}},{"cell_type":"code","source":"model4 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n    \n        Conv2D(64, (3, 3), padding='same', activation='relu'),\n        MaxPooling2D((2, 2)),\n\n        Flatten(),\n        Dense(312, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\n\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:26.342744Z","iopub.execute_input":"2021-12-16T21:27:26.343057Z","iopub.status.idle":"2021-12-16T21:27:26.406485Z","shell.execute_reply.started":"2021-12-16T21:27:26.343014Z","shell.execute_reply":"2021-12-16T21:27:26.405639Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model4.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n#Very cool early stopping and learning rate work\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), \n             ReduceLROnPlateau(patience=10, verbose=1)] ","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:26.409281Z","iopub.execute_input":"2021-12-16T21:27:26.409567Z","iopub.status.idle":"2021-12-16T21:27:26.420146Z","shell.execute_reply.started":"2021-12-16T21:27:26.409529Z","shell.execute_reply":"2021-12-16T21:27:26.419289Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"history4 = model4.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    \n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:27:26.421478Z","iopub.execute_input":"2021-12-16T21:27:26.421711Z","iopub.status.idle":"2021-12-16T21:41:24.808791Z","shell.execute_reply.started":"2021-12-16T21:27:26.421673Z","shell.execute_reply":"2021-12-16T21:41:24.807805Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plot_performance(history4, model4)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:41:24.810285Z","iopub.execute_input":"2021-12-16T21:41:24.810714Z","iopub.status.idle":"2021-12-16T21:41:28.014102Z","shell.execute_reply.started":"2021-12-16T21:41:24.810668Z","shell.execute_reply":"2021-12-16T21:41:28.013485Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Model 4 Evaluation\n\nIn Model 4 we continue to see improvements in every metric. At this point, this model is our best performing model. Model 4 has an accuracy of 99.69%, recall of 99.81%, and precision of 99.77%.","metadata":{}},{"cell_type":"markdown","source":"In Models 4, 5, and 6, additional convolutional and maxpooling layers were added. In each model I also increased the size of the first dense layer to see what effect it was having on overall performance. Models 4 and 5 can be found [here](https://www.kaggle.com/houleyemballo/total-models/edit). Each model showed slight improvement, but the most significant improvements were found in Models 4 and 6. Because of its simplicity, I finally chose to move forward with model 4. ","metadata":{}},{"cell_type":"markdown","source":"# Model 6: Adding Additional Layers","metadata":{}},{"cell_type":"markdown","source":"The inspiration for the majority of the layers in the final model was pulled from [this kaggle notebook](https://www.kaggle.com/jacobmorrison213/simple-cnn-using-keras). I was interested to compare this high performing model to my previous models.","metadata":{}},{"cell_type":"code","source":"model6 = Sequential([\n\n    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(64, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1, activation='sigmoid')\n    \n])\n\nmodel6.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:41:28.015013Z","iopub.execute_input":"2021-12-16T21:41:28.015222Z","iopub.status.idle":"2021-12-16T21:41:28.098475Z","shell.execute_reply.started":"2021-12-16T21:41:28.015196Z","shell.execute_reply":"2021-12-16T21:41:28.097511Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model6.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True),\n             ReduceLROnPlateau(patience=10, verbose=1),\n\n            ]","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:41:28.099805Z","iopub.execute_input":"2021-12-16T21:41:28.100017Z","iopub.status.idle":"2021-12-16T21:41:28.110579Z","shell.execute_reply.started":"2021-12-16T21:41:28.099992Z","shell.execute_reply":"2021-12-16T21:41:28.109736Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"history6 = model6.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    #class_weight = class_weights,\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-16T21:41:28.111829Z","iopub.execute_input":"2021-12-16T21:41:28.112031Z","iopub.status.idle":"2021-12-16T22:03:02.240179Z","shell.execute_reply.started":"2021-12-16T21:41:28.112005Z","shell.execute_reply":"2021-12-16T22:03:02.239081Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"\nplot_performance(history6, model6)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T22:03:02.241808Z","iopub.execute_input":"2021-12-16T22:03:02.242210Z","iopub.status.idle":"2021-12-16T22:03:06.234079Z","shell.execute_reply.started":"2021-12-16T22:03:02.242168Z","shell.execute_reply":"2021-12-16T22:03:06.233276Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Model 6 Evaluation\n\nModel 6 is performing almost as well as model 4, but it is slightly more complex. Because of this, it is best to move forward with the simplier model 4","metadata":{}},{"cell_type":"markdown","source":"# Evaluating Correct and Incorrect Predictions\n\nThis code was sourced from [an article](https://towardsdatascience.com/medical-x-ray-%EF%B8%8F-image-classification-using-convolutional-neural-network-9a6d33b1c2a) found on Towards Data Science.","metadata":{}},{"cell_type":"code","source":"preds = model6.predict(val_generator, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T22:03:06.235555Z","iopub.execute_input":"2021-12-16T22:03:06.235860Z","iopub.status.idle":"2021-12-16T22:03:08.286111Z","shell.execute_reply.started":"2021-12-16T22:03:06.235816Z","shell.execute_reply":"2021-12-16T22:03:08.285130Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"test = val_generator\n\ntest.reset()\nx=np.concatenate([test.next()[0] for i in range(test.__len__())])\ny=np.concatenate([test.next()[1] for i in range(test.__len__())])\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-16T22:03:08.287876Z","iopub.execute_input":"2021-12-16T22:03:08.288202Z","iopub.status.idle":"2021-12-16T22:03:09.999010Z","shell.execute_reply.started":"2021-12-16T22:03:08.288157Z","shell.execute_reply":"2021-12-16T22:03:09.997984Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"plot_model(model6,show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=True)\n\ndic = {0:'NO', 1:'Yes'}\nplt.figure(figsize=(20,20))\nfor i in range(0+228, 16+228):\n    \n    plt.subplot(4, 4, (i-228)+1)\n    if preds[i, 0] >= 0.5: \n        out = ('{:.2%} probability of HAVING cacti'.format(preds[i][0]))\n    else: \n        out = ('{:.2%} probability of NOT HAVING cacti'.format(1-preds[i][0]))\n    plt.title(out+\"\\n Cacti : \"+ dic.get(y[i]))    \n    plt.imshow(np.squeeze(x[i]))\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-16T22:03:10.000668Z","iopub.execute_input":"2021-12-16T22:03:10.000929Z","iopub.status.idle":"2021-12-16T22:03:12.236010Z","shell.execute_reply.started":"2021-12-16T22:03:10.000896Z","shell.execute_reply":"2021-12-16T22:03:12.233518Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"From the breakdown of predictions above, we can see that the model is very confident when it is making predictions. There are no predictions being made below %98.00. There are a couple of instances of incorrect predictions and in each case, it is obvious why the model is making a mistake in the prediction. Because of the low resolution in the photos, features that have a lot in common with the shape of columnar cactus are being mistaken for them. With higher resolution images, this could be improved upon, but our final model only misclassified 21 photos out of 3500.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"The final model chosen as the best performing model is Model 4, a CNN with two Conv2D layers, two MaxPooling layers, a flattening layer, and two dense layers. Early stop and Learning Rate Reduction was also used. \n\nThe final performance metric scores were 99.69% accuracy, 99.81% recall, and 99.77% precision.\n\nMy final model performs very well on unseen data and I am highly confident that my model would be well utilized on further unseen images. It would be an effective tool for the Mexican Government to begin their environmental site assessments using drones. I have iterated through many different models and I have also tried out a pre-trained Resnet model. The final model was chosen because it is simple, and also offers the highest performance. It is a good example of why added complexity does not always yield better results. \n\nWhile the results are good, they are not perfect. I do believe with further work it would be possible to have the model performing with 100% accuracy and %100 precision. I would like to move forward with a different type of activation layer to assess improvements. Pulling in other pre-trained models would also show if other types of pre-trained models show improvements that Resnet does not. \n\nOverall, a precision score of %99.77 and an overall accuracy of  means that we can identify areas that do not have our keystone species with confidence.","metadata":{}},{"cell_type":"markdown","source":"# Application of the Model\n\nOur model can be used for a wide variety of functions. First, it will be a large asset in resource prioritization. The care takers of the valley can perform drone flights over the entirety of the valley much faster than researchers could walk the space. The resulting images would be consistent, and areas lacking cacti can accurately be identified, helping pinpoint spaces for restoration work. This will cut down on any time wasted searching for new sites to amend. The use of a drone also helps monitor hard to reach areas. Finally, the use of a drone for monitoring is one of the least invasive ways to monitor natural areas. It reduces the need for automotive traffic and foot traffic through the region. This also helps with safety as researchers will not have to spend large amounts of time in remote areas in the field. ","metadata":{}},{"cell_type":"markdown","source":"# Next Steps","metadata":{}},{"cell_type":"markdown","source":"### Species classification expansion \n\nMoving forward, we would like to expand our model to recognize and classify other critical plant and animal species in the region. This would allow the model to give a complete ecological health account of the region. In the Tehuacán-Cuicatlán region, 70% of worldwide flora communities are represented with over 3,000 species of vascular plants of which %10 are endemic to the valley. By adding more species to our classification model, we can offer frequent drone flights to monitor the presence of more critical species in the area. Other species of critical import can be prioritized. There are 38 threatened species endemic to the region that could potentially be prioritized in further modeling. \n\n\n### Species Count \n\nTo make the model more robust, adding in the capacity to estimate the number of target species present in an image would help with any environmental site assessment. While it is useful to identify pieces of land with cacti present, it would be more useful to know the amount in an area. As our model stands now, a piece of land with a single cactus would be classified in the same category as a piece of land with 1500 cacti present. Adding the capacity for count would allow us to further prioritize land in need of remediation efforts. \n\n### Specific Human Impact Assessment \n\nRight now, our model is looking for the lack of certain critical plant species to evaluate the consequences of human impact. A better solution would be to train a model to specifically look for the hallmarks of negative human activity in sensitive areas like evidence of logging, mining, or agriculture. In images that are classified as having no cactus present, it would be useful to further classify them into human impact categories. ","metadata":{}},{"cell_type":"markdown","source":"# Research Citations\n\n- https://www.sciencedirect.com/science/article/abs/pii/S1574954119300895?via%3Dihub\n- https://jivg.org/research-projects/vigia/\n- https://www.researchgate.net/publication/237166397_Biotic_interactions_and_the_population_dynamics_of_the_long-lived_columnar_cactus_Neobuxbaumia_tetetzo_in_the_Tehuacan_Valley_Mexico\n- https://www.jstor.org/stable/3235892\n- https://www.nationalgeographic.org/encyclopedia/keystone-species/\n- https://whc.unesco.org/en/list/1534\n","metadata":{}}]}