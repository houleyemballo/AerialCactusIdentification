{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://ars.els-cdn.com/content/image/1-s2.0-S1574954119300895-gr1_lrg.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Overview","metadata":{}},{"cell_type":"markdown","source":"With the effects of climate change impacting the globe now more than ever, it is critical to maintain healthy natural areas as much as possible. Human activity has a large impact on environmental health, and tracking that impact is critical. Activities like logging, mining, and agriculture can destroy the functioning of natural areas.  In each ecosystem, there are plants and animals that have an outsized impact on the healthy balance found in the region. These keystone species are critical to the survival of many other species in the region. Performing an accurate account of the species present can give insight into the health of a given area, and can help pinpoint areas that are in special need of immediate restoration efforts. ","metadata":{}},{"cell_type":"markdown","source":"# Business Understanding","metadata":{}},{"cell_type":"markdown","source":"The Mexican Government has partnered with the VIGIA project, which aims to create an autonomous surveillance system for monitoring the ecological health of protected natural areas. The first step in this project is to use drone footage to identify critical, or keystone species in the images. The initial images are from the Tehuac치n-Cuicatl치n Valley, a semi-arid zone in southern Mexico. This area was chosen because of its wide biodiversity and the importance that the ecology plays in the health of the region. It became a UNESCO World Heritage Site in 2018 in an effort to protect the area. Despite this protection, human activity is still impacting the health of the region. \n\n## Ecological Understanding\n\nOur work focuses on columnar cactus recognition as they are a keystone species in the region. \n\nCare must be taken to ensure that images labeled as having cactus present are accurate. While it would cause a waste of labor to move to intervene in an area that was incorrectly marked as not having cacti, it would be more problematic to miss impacted areas all together. This would mean areas that are in need of restoration would go unnoticed. \n","metadata":{}},{"cell_type":"code","source":"#Import Relevant Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Input, LSTM, AveragePooling2D\nfrom keras.datasets import mnist\nfrom keras import regularizers, initializers, optimizers\nimport os\nimport datetime\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import plot_confusion_matrix\nfrom pathlib import Path\nimport matplotlib.image as mpimg\nfrom sklearn.metrics import roc_auc_score, plot_confusion_matrix\nimport seaborn as sns\nimport gc\nfrom timeit import default_timer as timer\nimport tensorflow as tf\npd.set_option('display.float_format', lambda x: '%.1f' % x)\nos.environ['KMP_DUPLICATE_LIB_OK']='True' #This prevents kernel shut down due to xgboost conflict\nfrom tensorflow.keras.preprocessing import image\nimport zipfile\nimport random\nimport cv2\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.layers import BatchNormalization, Reshape, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport pydot\nimport graphviz\nfrom tensorflow.keras.utils import plot_model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Take a look at the available data\nprint(os.listdir(\"../input/aerial-cactus-identification/\"))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:38:51.335858Z","iopub.execute_input":"2021-12-09T01:38:51.336484Z","iopub.status.idle":"2021-12-09T01:38:58.956332Z","shell.execute_reply.started":"2021-12-09T01:38:51.336375Z","shell.execute_reply":"2021-12-09T01:38:58.955589Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\ndef plot_performance(hist):\n    \"\"\" Returns 5 pairs of plots comparing Training and Validation data. \n    The first column is autoscaled. The second column has a set scale.\n    First pair of plots returns training and validation accuracy. \n    Second pair of plots returns training and validation loss. \n    Third pair of plots returns training and validation F1-Scores. \n    Fourth pair of plots returns training and validation recall scores. \n    Fifth pair of plots returns training and validation precision scores.\n    \n    hist: input history model containing train images, labels, and validation data. \"\"\"\n    \n    hist_ = hist.history\n    epochs = hist.epoch\n    recall = np.array(hist_['recall'])\n    precision = np.array(hist_['precision'])\n    val_recall = np.array(hist_['val_recall'])\n    val_precision = np.array(hist_['val_precision'])\n    \n    #\n    figure,axis = plt.subplots(5,2, figsize = (30,30))\n    #Accuracy plots\n    axis[0,0].plot(epochs, hist_['accuracy'], label='Training Accuracy')\n    axis[0,0].plot(epochs, hist_['val_accuracy'], label='Validation Accuracy')\n    \n    axis[0,0].set_title('Training and validation accuracy')\n    axis[0,0].legend()\n    \n    \n    axis[0,1].plot(epochs, hist_['accuracy'], label='Training Accuracy')\n    axis[0,1].plot(epochs, hist_['val_accuracy'], label='Validation Accuracy')\n    axis[0,1].set_ylim(0,1)\n    axis[0,1].set_title('Training and validation accuracy')\n    axis[0,1].legend()\n    \n    \n    #--------------------------------------------------------------------\n    #Loss\n    axis[1,0].plot(epochs, hist_['loss'], label='Training loss')\n    axis[1,0].plot(epochs, hist_['val_loss'], label='Validation loss')\n    \n    axis[1,0].set_title('Training and validation loss')\n    axis[1,0].legend()\n    \n    \n    axis[1,1].plot(epochs, hist_['loss'], label='Training loss')\n    axis[1,1].plot(epochs, hist_['val_loss'], label='Validation loss')\n    axis[1,1].set_ylim(0,1)\n    axis[1,1].set_title('Training and validation loss')\n    axis[1,1].legend()\n    \n    #-------------------------------------------------------------------\n    #F1 Scores\n    axis[2,0].plot(epochs, \n             2*((recall * precision)/(recall + precision)), \n             label='Training f1')\n    axis[2,0].plot(epochs, \n             2*((val_recall * val_precision)/(val_recall + val_precision)), \n             label='Validation f1')\n    \n    axis[2,0].set_title('Training and validation F1-Score')\n    axis[2,0].legend()\n    \n    \n    axis[2,1].plot(epochs, \n             2*((recall * precision)/(recall + precision)), \n             label='Training f1')\n    axis[2,1].plot(epochs, \n             2*((val_recall * val_precision)/(val_recall + val_precision)), \n             label='Validation f1')\n    axis[2,1].set_ylim(0,1)\n    axis[2,1].set_title('Training and validation F1-Score')\n    axis[2,1].legend()\n    \n    #-------------------------------------------------------------------------\n    #Recall\n    \n    \n    axis[3,0].plot(epochs, recall, label = \"Training Recall\")\n    axis[3,0].plot(epochs, val_recall, label = \"Validation Recall\")\n    \n    axis[3,0].set_title(\"Training and Validation Recall Scores\")\n    axis[3,0].legend()\n    \n    \n    axis[3,1].plot(epochs, recall, label = \"Training Recall\")\n    axis[3,1].plot(epochs, val_recall, label = \"Validation Recall\")\n    axis[3,1].set_ylim(0,1)\n    axis[3,1].set_title(\"Training and Validation Recall Scores\")\n    axis[3,1].legend()\n    \n    #------------------------------------------------------------------------\n    #Precision\n    axis[4,0].plot(epochs, precision, label = \"Training Precision\")\n    axis[4,0].plot(epochs, val_precision, label = \"Validation Precision\")\n    \n    axis[4,0].set_title(\"Training and Validation Precision Scores\")\n    axis[4,0].legend()\n    \n    \n    \n    axis[4,1].plot(epochs, precision, label = \"Training Precision\")\n    axis[4,1].plot(epochs, val_precision, label = \"Validation Precision\")\n    axis[4,1].set_ylim(0,1)\n    axis[4,1].set_title(\"Training and Validation Precision Scores\")\n    axis[4,1].legend()\n    \n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:38:58.957925Z","iopub.execute_input":"2021-12-09T01:38:58.958159Z","iopub.status.idle":"2021-12-09T01:38:58.977398Z","shell.execute_reply.started":"2021-12-09T01:38:58.958132Z","shell.execute_reply":"2021-12-09T01:38:58.976754Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Understanding","metadata":{}},{"cell_type":"markdown","source":"The original data found in this notebook was generated by the research team, VIGIA. Using drone footage, the team collected over 17,000 images of a 10,000 square km area of the Tehuac치n-Cuicatl치n Valley in Southern Mexico. The dataset conatins a collection of 32 x 32 thumbnail images. The images used in thie notebook were resized but the original images can be found [here.](https://www.kaggle.com/irvingvasquez/cactus-aerial-photos)","metadata":{}},{"cell_type":"markdown","source":"First, we exract our images from their zip files, and save them to a temporary directory for ease of working. We then can take a look at how many images are present in both our train and test files. ","metadata":{}},{"cell_type":"code","source":"# Extract images from zip files\n\nwith zipfile.ZipFile(\"../input/aerial-cactus-identification/train.zip\",\"r\") as z:\n    z.extractall(\"/kaggle/temp/\")\nwith zipfile.ZipFile(\"../input/aerial-cactus-identification/test.zip\",\"r\") as z:\n    z.extractall(\"/kaggle/temp/test/\") # needs to be in subdirectory (i.e. test/test/) for flow_from_directory to work\n\n\n#Looking at the number of images in our train and test files\nprint(len(os.listdir(\"../temp/train\")))\nprint(len(os.listdir(\"../temp/test/test\")))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:38:58.978666Z","iopub.execute_input":"2021-12-09T01:38:58.978927Z","iopub.status.idle":"2021-12-09T01:39:02.303461Z","shell.execute_reply.started":"2021-12-09T01:38:58.978899Z","shell.execute_reply":"2021-12-09T01:39:02.302545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Setting up our working directories\ntrain_dir = \"../temp/train\"\ntest_dir = \"../temp/test\"\nlabels = pd.read_csv('../input/aerial-cactus-identification/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:02.305239Z","iopub.execute_input":"2021-12-09T01:39:02.305464Z","iopub.status.idle":"2021-12-09T01:39:02.341508Z","shell.execute_reply.started":"2021-12-09T01:39:02.305437Z","shell.execute_reply":"2021-12-09T01:39:02.340625Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Image Analysis Overview","metadata":{}},{"cell_type":"markdown","source":"We can see from the count, and visualized in the graph below, the classes are very imbalanced. There are three times as many cacti images as there are no cacti images. We will later address this imbalance by augmenting the photos.","metadata":{}},{"cell_type":"code","source":"#Taking a look at the class balance \nlabels.has_cactus = labels.has_cactus.astype(str)\nprint(labels['has_cactus'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:02.342903Z","iopub.execute_input":"2021-12-09T01:39:02.343617Z","iopub.status.idle":"2021-12-09T01:39:02.388219Z","shell.execute_reply.started":"2021-12-09T01:39:02.343583Z","shell.execute_reply":"2021-12-09T01:39:02.387181Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"sns.histplot(data = labels['has_cactus'], shrink = .8)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:02.389398Z","iopub.execute_input":"2021-12-09T01:39:02.389768Z","iopub.status.idle":"2021-12-09T01:39:02.626359Z","shell.execute_reply.started":"2021-12-09T01:39:02.389736Z","shell.execute_reply":"2021-12-09T01:39:02.625558Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We can also take a look at a random sampling of the training images in our dataset. We can see that the images are not of the highest resolution which is to be expected from images taken from 100m, but the cacti are distinct.","metadata":{}},{"cell_type":"markdown","source":"This segment of code was sourced from this [kaggle user](https://www.kaggle.com/jacobmorrison213/simple-cnn-using-keras) to look through a random set of images. ","metadata":{}},{"cell_type":"code","source":"# Plot random sample of training images\n\nrand_images = random.sample(os.listdir(train_dir), 16)\n\nfig = plt.figure(figsize=(16,4))\nfor i, im in enumerate(rand_images):\n    plt.subplot(2, 8, i+1)\n    im = cv2.imread(os.path.join(train_dir, im))\n    plt.imshow(im)\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:02.627596Z","iopub.execute_input":"2021-12-09T01:39:02.628146Z","iopub.status.idle":"2021-12-09T01:39:03.536488Z","shell.execute_reply.started":"2021-12-09T01:39:02.628108Z","shell.execute_reply":"2021-12-09T01:39:03.535791Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ex_split = 0.8\nindex = np.random.permutation(range(len(labels))) < ex_split*len(labels)\n\nex_labels = labels[index]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:03.537407Z","iopub.execute_input":"2021-12-09T01:39:03.538293Z","iopub.status.idle":"2021-12-09T01:39:03.554399Z","shell.execute_reply.started":"2021-12-09T01:39:03.538246Z","shell.execute_reply":"2021-12-09T01:39:03.553420Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"ex_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255, horizontal_flip=True, vertical_flip=True)\n\nbatch_size = 128\n\nex_generator = ex_datagen.flow_from_dataframe(ex_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:03.555625Z","iopub.execute_input":"2021-12-09T01:39:03.555984Z","iopub.status.idle":"2021-12-09T01:39:03.701617Z","shell.execute_reply.started":"2021-12-09T01:39:03.555954Z","shell.execute_reply":"2021-12-09T01:39:03.700862Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"ex_images, ex_labels = next(ex_generator)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:03.703998Z","iopub.execute_input":"2021-12-09T01:39:03.704200Z","iopub.status.idle":"2021-12-09T01:39:03.741658Z","shell.execute_reply.started":"2021-12-09T01:39:03.704175Z","shell.execute_reply":"2021-12-09T01:39:03.740753Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.imshow(array_to_img(ex_images[0]))\nprint(ex_labels[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:03.743372Z","iopub.execute_input":"2021-12-09T01:39:03.743666Z","iopub.status.idle":"2021-12-09T01:39:03.941761Z","shell.execute_reply.started":"2021-12-09T01:39:03.743627Z","shell.execute_reply":"2021-12-09T01:39:03.941046Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.imshow(array_to_img(ex_images[2]))\nprint(ex_labels[2])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:03.942840Z","iopub.execute_input":"2021-12-09T01:39:03.943077Z","iopub.status.idle":"2021-12-09T01:39:04.139359Z","shell.execute_reply.started":"2021-12-09T01:39:03.943050Z","shell.execute_reply":"2021-12-09T01:39:04.138444Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Performing a train test split","metadata":{}},{"cell_type":"markdown","source":"Here we split our data into our training set and our validation set. 80% of the images are going into the training set with the remaining images being used to validate our models performance.","metadata":{}},{"cell_type":"code","source":"#Splits the data randomly\nval_split = 0.8\nindex = np.random.permutation(range(len(labels))) < val_split*len(labels)\n\ntrain_labels = labels[index]\nval_labels = labels[~index]\nprint(len(train_labels), len(val_labels))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.141121Z","iopub.execute_input":"2021-12-09T01:39:04.141442Z","iopub.status.idle":"2021-12-09T01:39:04.157157Z","shell.execute_reply.started":"2021-12-09T01:39:04.141401Z","shell.execute_reply":"2021-12-09T01:39:04.156089Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Image Data Pre-Processing","metadata":{}},{"cell_type":"markdown","source":"We want to start pre-processing our images before putting them through the models. Our pre-processing will ensure all images are the same size and are scaled down to relieve any computational pressure. In the initial image data generator, we flip our images horizontally and vertically to give our model more images from which to train. We then use flow from dataframe to utilize the augmentation from our image data generator.\n\nAs we set up our train and validation generators, we have to make sure to specify our batch size and target size. It is important as well to indicate that we are working on a binary classification problem by setting class_mode to 'binary'.","metadata":{}},{"cell_type":"code","source":"# Process image JPEGs into tensors\n# Pixel values rescaled from [0,255] to [0,1]\n\n# Generate batches of tensor image data (with real-time data augmentation - horizontal and vertical flips)\ntrain_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255, horizontal_flip=True, vertical_flip=True)\n\nbatch_size = 128\n\ntrain_generator = train_datagen.flow_from_dataframe(train_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))\nval_generator = train_datagen.flow_from_dataframe(val_labels,directory=train_dir,x_col='id',\n                                                    y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                                    target_size=(32,32))","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.158581Z","iopub.execute_input":"2021-12-09T01:39:04.159101Z","iopub.status.idle":"2021-12-09T01:39:04.341481Z","shell.execute_reply.started":"2021-12-09T01:39:04.159062Z","shell.execute_reply":"2021-12-09T01:39:04.340615Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We can now set our input shape that will be refelective of the target_size given above, and also the number of layers found in our images. Because we are working with RGB images, our layers are set to 3.","metadata":{}},{"cell_type":"code","source":"input_shape = (32,32,3)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.342578Z","iopub.execute_input":"2021-12-09T01:39:04.342897Z","iopub.status.idle":"2021-12-09T01:39:04.347069Z","shell.execute_reply.started":"2021-12-09T01:39:04.342858Z","shell.execute_reply":"2021-12-09T01:39:04.346254Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"markdown","source":"In the first model, we would like to start simply to see what kind of results we can get with a little complication as possible. \n\nWe start with an input layer to establish the structure of the incoming data. We then include a flattening layer to set our images into one long array. Two simple dense layers are then added which funnel the information down. The first dense layer uses a relu activation function. Our final dense layer uses a sigmoid activation function because we are working on a binary classification problem.","metadata":{}},{"cell_type":"code","source":"#First simple model\nmodel_start = Sequential(\n    [\n        Input(input_shape),\n        Flatten(), # need to flatten our images to be one long array\n        Dense(64,activation=\"relu\"),\n        Dense(1, activation=\"sigmoid\"),  \n        \n    ])\n\nmodel_start.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.348137Z","iopub.execute_input":"2021-12-09T01:39:04.349280Z","iopub.status.idle":"2021-12-09T01:39:04.467514Z","shell.execute_reply.started":"2021-12-09T01:39:04.349244Z","shell.execute_reply":"2021-12-09T01:39:04.466499Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We now compile our model using binary crossentropy to evaluate our loss. We also use adam as our optimizer and evaluate using the accuracy, recall and precision metrics.. ","metadata":{}},{"cell_type":"code","source":"model_start.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy', 'Recall', 'Precision'])","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.468984Z","iopub.execute_input":"2021-12-09T01:39:04.469306Z","iopub.status.idle":"2021-12-09T01:39:04.482736Z","shell.execute_reply.started":"2021-12-09T01:39:04.469266Z","shell.execute_reply":"2021-12-09T01:39:04.481986Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"We now can move to fitting our baseline model. We will set the number of epochs to 20 to get a quick understanding of our models performance. ","metadata":{}},{"cell_type":"code","source":"history = model_start.fit(train_generator, \n                     batch_size=batch_size,\n                     epochs=20, \n                     validation_data=val_generator)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:39:04.484358Z","iopub.execute_input":"2021-12-09T01:39:04.484866Z","iopub.status.idle":"2021-12-09T01:40:53.265984Z","shell.execute_reply.started":"2021-12-09T01:39:04.484795Z","shell.execute_reply":"2021-12-09T01:40:53.265335Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"score1 = model_start.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score1[0])\nprint(\"Test accuracy:\", score1[1])\nprint(\"Test recall:\", score1[2])\nprint(\"Test precision:\", score1[3])\n\nplot_performance(history)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:40:53.267088Z","iopub.execute_input":"2021-12-09T01:40:53.267436Z","iopub.status.idle":"2021-12-09T01:40:55.858673Z","shell.execute_reply.started":"2021-12-09T01:40:53.267409Z","shell.execute_reply":"2021-12-09T01:40:55.857809Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"Our first simple model has an overall test accuracy of 82%. Our test recall is 85%. Most importantly, our precision is 91%. This is a great start for our modeling. Precision is our most important metric because it is evaluating how often an image is accurately reporting that cacti are present. We need to be able to trust this metric because a mislabeling of a land as having cacti will lead to the land getting ignored. ","metadata":{}},{"cell_type":"markdown","source":"# Model Iterations","metadata":{}},{"cell_type":"markdown","source":"In model two, we wanted to play around with diferent types of layers to see their effects.\n\nOur first layer is a Conv2D layer using the relu activation function. Relu is useful over tanh and sigmoid in the topmost layers because it makes the model easier to train and often the model achieves better performance. Setting 'same' to padding extends the area in which CNN processes an image. We also make sure to set our input shape. Our initial filter is set in this layer which determines the number of filters from which the CNN will learn.\n\nWe then include a MaxPooling2D layer. This operation is putting a 2D filter over each channel and summarising the features covered by the filter. This is performing downsampling on our photos, specifically on the height and width of the spatial dimensions.\n\nWe then flatten our matrix down to a single long array before putting it through two more Dense layers.\n\nIn compiling our model we are using an adam optimizer with a set learning rate of .01. Additionally, we have put in an early stop in this model. Early stop will make sure to stop the model learning at a point where it seems there are no more improvements to be made on model function.\n\nWe are giving the next models more epochs to run through to evaluate performance over more iterations. We can set the epoch to a high number, and trust that the early stop will end the model training when no improvements are being made. ","metadata":{}},{"cell_type":"markdown","source":"## Model 2: Adding Convolutional Layers and MaxPooling","metadata":{}},{"cell_type":"code","source":"model2 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\nopt = keras.optimizers.Adam(learning_rate=0.01)\n\n# compiling models\nmodel2.compile(loss='binary_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy', 'Recall', 'Precision'])\n\n# early stopping\ncp = EarlyStopping(patience = 20, restore_best_weights=True)\n\nmodel2.summary()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-09T01:40:55.860078Z","iopub.execute_input":"2021-12-09T01:40:55.860937Z","iopub.status.idle":"2021-12-09T01:40:55.918590Z","shell.execute_reply.started":"2021-12-09T01:40:55.860899Z","shell.execute_reply":"2021-12-09T01:40:55.917743Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"start = timer()\n\nhistory2 = model2.fit(train_generator,\n                    batch_size = batch_size,\n                    epochs=50, \n                    validation_data = val_generator,\n                    callbacks = [cp]\n                    \n                  \n                   )\nend = timer()\nelapsed = end - start\nprint('Total Time Elapsed: ', int(elapsed//60), ' minutes ', (round(elapsed%60)), ' seconds')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:40:55.920247Z","iopub.execute_input":"2021-12-09T01:40:55.920815Z","iopub.status.idle":"2021-12-09T01:47:07.820478Z","shell.execute_reply.started":"2021-12-09T01:40:55.920774Z","shell.execute_reply":"2021-12-09T01:47:07.819478Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"score2 = model2.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score2[0])\nprint(\"Test accuracy:\", score2[1])\nprint(\"Test recall:\", score2[2])\nprint(\"Test precision:\", score2[3])\nplot_performance(history2)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:47:07.821954Z","iopub.execute_input":"2021-12-09T01:47:07.822354Z","iopub.status.idle":"2021-12-09T01:47:10.665845Z","shell.execute_reply.started":"2021-12-09T01:47:07.822309Z","shell.execute_reply":"2021-12-09T01:47:10.664955Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Model 2 Evaluation ","metadata":{}},{"cell_type":"markdown","source":"Model 2 has an overall test accuracy of 96%. Our test recall is 97%. Most importantly, our precision is 97%. This is a massive improvement on our first model but we are seeing the recall scores jump all over the place between 91% and 98%. This could be a sign we need to let our models train for longer so in future iterations we will increase the number of epochs to 100. ","metadata":{}},{"cell_type":"markdown","source":"## Model 3: Learning Rate Reductions and Early Stopping","metadata":{}},{"cell_type":"markdown","source":"In model 3, we are keeping the same layers for a our model, but we are making some changes to the early stopping and adding in a feature that allows the learning rate to gradually adjust as needed. We are also going to allow the model to train for 100 epochs. ","metadata":{}},{"cell_type":"code","source":"model3 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\n\nmodel3.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:47:10.667109Z","iopub.execute_input":"2021-12-09T01:47:10.667312Z","iopub.status.idle":"2021-12-09T01:47:10.714128Z","shell.execute_reply.started":"2021-12-09T01:47:10.667287Z","shell.execute_reply":"2021-12-09T01:47:10.713266Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model3.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n#Very cool early stopping and learning rate work\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), # Stop training when a monitored metric has stopped improving\n             ReduceLROnPlateau(patience=10, verbose=1), # Reduce learning rate when a metric has stopped improving\n\n            ]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:47:10.715516Z","iopub.execute_input":"2021-12-09T01:47:10.716890Z","iopub.status.idle":"2021-12-09T01:47:10.728165Z","shell.execute_reply.started":"2021-12-09T01:47:10.716843Z","shell.execute_reply":"2021-12-09T01:47:10.727374Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"history3 = model3.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    \n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:47:10.729709Z","iopub.execute_input":"2021-12-09T01:47:10.730161Z","iopub.status.idle":"2021-12-09T01:56:19.966710Z","shell.execute_reply.started":"2021-12-09T01:47:10.730129Z","shell.execute_reply":"2021-12-09T01:56:19.965250Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"score3 = model3.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score3[0])\nprint(\"Test accuracy:\", score3[1])\nprint(\"Test recall:\", score3[2])\nprint(\"Test precision:\", score3[3])\nplot_performance(history3)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:56:19.969034Z","iopub.execute_input":"2021-12-09T01:56:19.969432Z","iopub.status.idle":"2021-12-09T01:56:22.712544Z","shell.execute_reply.started":"2021-12-09T01:56:19.969381Z","shell.execute_reply":"2021-12-09T01:56:22.711667Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Model 4","metadata":{}},{"cell_type":"code","source":"model4 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n    \n        Conv2D(64, (3, 3), padding='same', activation='relu'),\n        MaxPooling2D((2, 2)),\n\n        Flatten(),\n        Dense(312, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\n\nmodel4.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:02:07.471528Z","iopub.execute_input":"2021-12-09T02:02:07.472324Z","iopub.status.idle":"2021-12-09T02:02:07.536298Z","shell.execute_reply.started":"2021-12-09T02:02:07.472266Z","shell.execute_reply":"2021-12-09T02:02:07.535453Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model4.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n#Very cool early stopping and learning rate work\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), # Stop training when a monitored metric has stopped improving\n             ReduceLROnPlateau(patience=10, verbose=1)] # Reduce learning rate when a metric has stopped improving","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:02:30.650495Z","iopub.execute_input":"2021-12-09T02:02:30.651405Z","iopub.status.idle":"2021-12-09T02:02:30.661327Z","shell.execute_reply.started":"2021-12-09T02:02:30.651347Z","shell.execute_reply":"2021-12-09T02:02:30.660675Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"history4 = model4.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    \n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:02:32.935823Z","iopub.execute_input":"2021-12-09T02:02:32.936794Z","iopub.status.idle":"2021-12-09T02:15:00.055832Z","shell.execute_reply.started":"2021-12-09T02:02:32.936753Z","shell.execute_reply":"2021-12-09T02:15:00.055045Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"score4 = model4.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score4[0])\nprint(\"Test accuracy:\", score4[1])\nprint(\"Test recall:\", score4[2])\nprint(\"Test precision:\", score4[3])\nplot_performance(history4)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:43:57.120185Z","iopub.execute_input":"2021-12-09T02:43:57.120492Z","iopub.status.idle":"2021-12-09T02:44:00.405734Z","shell.execute_reply.started":"2021-12-09T02:43:57.120461Z","shell.execute_reply":"2021-12-09T02:44:00.404770Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"# Model 5","metadata":{}},{"cell_type":"code","source":"model5 = Sequential([\n\n        Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape = input_shape),\n        MaxPooling2D((2, 2)),\n    \n        \n        Conv2D(64, (3, 3), padding='same', activation='relu'),\n        MaxPooling2D((2, 2)),\n    \n        Flatten(),\n        Dense(412, activation = 'relu'),\n        Dense(1, activation='sigmoid')\n\n]\n)\n\nmodel5.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:15:02.953339Z","iopub.execute_input":"2021-12-09T02:15:02.953646Z","iopub.status.idle":"2021-12-09T02:15:03.019060Z","shell.execute_reply.started":"2021-12-09T02:15:02.953605Z","shell.execute_reply":"2021-12-09T02:15:03.018203Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model5.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n#Very cool early stopping and learning rate work\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), # Stop training when a monitored metric has stopped improving\n             ReduceLROnPlateau(patience=20, verbose=1)] # Reduce learning rate when a metric has stopped improving","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:15:03.021619Z","iopub.execute_input":"2021-12-09T02:15:03.022248Z","iopub.status.idle":"2021-12-09T02:15:03.031836Z","shell.execute_reply.started":"2021-12-09T02:15:03.022200Z","shell.execute_reply":"2021-12-09T02:15:03.030963Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"history5 = model5.fit(train_generator,\n                    epochs = 150,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    \n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:15:03.032949Z","iopub.execute_input":"2021-12-09T02:15:03.033264Z","iopub.status.idle":"2021-12-09T02:26:41.549696Z","shell.execute_reply.started":"2021-12-09T02:15:03.033213Z","shell.execute_reply":"2021-12-09T02:26:41.548399Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"score5 = model5.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score5[0])\nprint(\"Test accuracy:\", score5[1])\nprint(\"Test recall:\", score5[2])\nprint(\"Test precision:\", score5[3])\nplot_performance(history5)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:26:41.550884Z","iopub.execute_input":"2021-12-09T02:26:41.551258Z","iopub.status.idle":"2021-12-09T02:26:44.840016Z","shell.execute_reply.started":"2021-12-09T02:26:41.551226Z","shell.execute_reply":"2021-12-09T02:26:44.839191Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Model 6: Adding Additional Layers","metadata":{}},{"cell_type":"markdown","source":"The inspiration for the majority of the layers in the final model was pulled from [this kaggle notebook](https://www.kaggle.com/jacobmorrison213/simple-cnn-using-keras)","metadata":{}},{"cell_type":"code","source":"\n\nmodel6 = Sequential([\n\n    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(64, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Conv2D(128, (3, 3), padding='same', activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1, activation='sigmoid')\n    \n])\n\nmodel6.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:26:44.841123Z","iopub.execute_input":"2021-12-09T02:26:44.841410Z","iopub.status.idle":"2021-12-09T02:26:44.919771Z","shell.execute_reply.started":"2021-12-09T02:26:44.841380Z","shell.execute_reply":"2021-12-09T02:26:44.918952Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model6.compile(loss = keras.losses.binary_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy', 'Recall', 'Precision'])\n\ncp = [EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True), # Stop training when a monitored metric has stopped improving\n             ReduceLROnPlateau(patience=10, verbose=1), # Reduce learning rate when a metric has stopped improving\n\n            ]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:26:44.920977Z","iopub.execute_input":"2021-12-09T02:26:44.921202Z","iopub.status.idle":"2021-12-09T02:26:44.930625Z","shell.execute_reply.started":"2021-12-09T02:26:44.921174Z","shell.execute_reply":"2021-12-09T02:26:44.929906Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"history6 = model6.fit(train_generator,\n                    epochs = 100,\n                    verbose = 1,\n                    callbacks = cp,\n                    validation_data = val_generator,\n                    #class_weight = class_weights,\n                   )","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:26:44.933528Z","iopub.execute_input":"2021-12-09T02:26:44.933905Z","iopub.status.idle":"2021-12-09T02:42:03.791791Z","shell.execute_reply.started":"2021-12-09T02:26:44.933866Z","shell.execute_reply":"2021-12-09T02:42:03.790978Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"score6 = model6.evaluate(val_generator, verbose=0)\nprint(\"Test loss:\", score6[0])\nprint(\"Test accuracy:\", score6[1])\nprint(\"Test recall:\", score6[2])\nprint(\"Test precision:\", score6[3])\n\nplot_performance(history6)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:42:03.796029Z","iopub.execute_input":"2021-12-09T02:42:03.796264Z","iopub.status.idle":"2021-12-09T02:42:07.182335Z","shell.execute_reply.started":"2021-12-09T02:42:03.796232Z","shell.execute_reply":"2021-12-09T02:42:07.181361Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"preds = model6.predict(val_generator, verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:42:07.183675Z","iopub.execute_input":"2021-12-09T02:42:07.184016Z","iopub.status.idle":"2021-12-09T02:42:09.085000Z","shell.execute_reply.started":"2021-12-09T02:42:07.183977Z","shell.execute_reply":"2021-12-09T02:42:09.084107Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"test = val_generator\n\ntest.reset()\nx=np.concatenate([test.next()[0] for i in range(test.__len__())])\ny=np.concatenate([test.next()[1] for i in range(test.__len__())])\nprint(x.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:42:09.086122Z","iopub.execute_input":"2021-12-09T02:42:09.087004Z","iopub.status.idle":"2021-12-09T02:42:10.747611Z","shell.execute_reply.started":"2021-12-09T02:42:09.086959Z","shell.execute_reply":"2021-12-09T02:42:10.746626Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"plot_model(model6,show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=True)\n\ndic = {0:'NO', 1:'Yes'}\nplt.figure(figsize=(20,20))\nfor i in range(0+228, 16+228):\n    \n    plt.subplot(4, 4, (i-228)+1)\n    if preds[i, 0] >= 0.5: \n        out = ('{:.2%} probability of HAVING cacti'.format(preds[i][0]))\n    else: \n        out = ('{:.2%} probability of NOT HAVING cacti'.format(1-preds[i][0]))\n    plt.title(out+\"\\n Cacti : \"+ dic.get(y[i]))    \n    plt.imshow(np.squeeze(x[i]))\n    plt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T02:42:10.748863Z","iopub.execute_input":"2021-12-09T02:42:10.749066Z","iopub.status.idle":"2021-12-09T02:42:12.900634Z","shell.execute_reply.started":"2021-12-09T02:42:10.749041Z","shell.execute_reply":"2021-12-09T02:42:12.899616Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"Our final model performs very well on unseen data and we are highly confident that our model is generalizable to uneen photos and would be an effect tool for the Mexican Government to begin their environmental site assessments using drones.","metadata":{}},{"cell_type":"markdown","source":"## Next Steps","metadata":{}},{"cell_type":"markdown","source":"Species classification expansion - Moving forward, we would like to expand our model to recognize and classify other critical plant and animal species in the region. This would allow the model to give a complete ecological health account of the region.\n\nSpecific Human Impact Assessment - Right now, our model is looking for the lack of certain critical plant species to evaluate the consequences of human impact. A better solution would be to train a model to specifically look for the hallmarks of negative human activity in sensitive areas like evidence of logging, mining, or agriculture.\n\nSpecies Count - To make the model more robust, adding in the capacity to estimate the number of taget species present in an image would help with any enviornmental site assessment.","metadata":{}}]}